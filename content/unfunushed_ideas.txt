

### Philosofical Interlude: Does Information Exist?

Chapter Draft (everything in this draft is very vague, dont read)
- make reader know that you are critic of your own ideas
 - Previous Section information relied on an assumed probability space
 - what is probability? It is a way to formally express of what can be known and what can not be known? (<- express waeknasses of that thought and derive it and be more precise and present alternatives of expressong of what can be known and what not)
- Problems of probability:
    - How well can be known what can not be know? Well enough to express probabilitys? (name logic example to define what can be known and what can be not and probabilistic example, and argue whether there is something inbetween)

    - maybe also everything can be known and everything happens with probability one (<- drop argument, why one can not be sure that universe is not deterministic universe, by using argument based on an explanation why even anything exists)
        - explain that under that circumstances only physically isolated can be known, using similar "proof" as holding theorems proof
        - include limitations of calculation power into probability?
- use previous arguments to express where probabilistic assumptions might  be off and name examples on how it could affect us practically
- conclude if we are carefull enough with probabilistic assumptions under scenario a everything has information 0, under scenario b everything has information $\infty$ and under scenario a using limitation of physical possible calculation power information content is impossibly hard to compute
- mention that information theory has been usefull anyway




#### When can we make assumptions of the the familie of AMs

Lets suppose following scenario: we have a sequence of random tokens $X_1, ..., X_T$ and know that this sequence is autoregressive with finite horizon T-d, which means $\mathbb P(X_{t+1} | X_{t-T+d}, ..., X_{t-1}) = \mathbb P(X_{t+1}|H_t, X_{t+2}, ...X_{T})$.
Then, the distribution of $H_T$ can be charachterized by the function $f(H_t) = \mathbb P(X_{t+1} | H_t)$. Thus there is a trivial bijection from the set $\{ \Sigma^{T-d} \to \mathcal P_\Sigma \}$ to the Autoregressive distributions over the alphabet $\Sigma$ and length $T$ and horizon $T-d$. (<- warning you have to integrate the non character sign to sigma and that can only be place to the right position (change $\Sigma^T$)).

Lets suppose we are reely lucky and  find a parmatrization of all autoregressive distributions of horizon T-d by finding an injektive function $A :\mathbb J_\sigma^n \to \{\Sigma^{T-d} \to \mathcal P_\Sigma \}$, where $\mathbb J_\sigma ^n := \{||x||_2 = \sigma, \mathbb{1} ^T x=0 | x \in \mathbb R^n\}$. (injecion exist since in and output sets are the same size)

(

Let now $\Theta \sim \mathcal N(0,I_n\sigma)$. Then $A(\Theta)$ is a random random distribution which itself is a random distribution. This means $A(\Theta) \in \{\Sigma^T \to \mathcal P_\Sigma \}$ (Note $X \sim \mathcal N(0,1) \to x \notin \mathbb R$) 



<!--This means there must exits $\theta \in \mathbb J^n$, such that  $A(\theta) = A^{-1}(A(\Theta))$-->

<!--Try to find argument that autogenerative model can not create itsel, can not be a finite horizon thing, but I think that is going nowhere-->


If A exists, and we  insert random vector into A, then that random variable is element of As image

for every random bayesian prior there exist a constant bayesian prior!!

(!!for every constant bayesian prior  model there exists another model with non constant bayesian prior (just add another theta i and make x independent of it, trivial bayesian + model modification)!!!)

but not for every distribution there is a bayesian prior, such that estimation error bigger than zero (<- thats the actual inverse)hehehe



lets find K that maps every distribution of theta to its kernel theta

e.g. K: random distribution -> constant

$A(K(l(\Theta)) = A(\Theta)$

<!--Say that iid if theta is not the worst case assumption for our model.-->

To recapulate, we assumed/showed evidence, that there can exist a pair $A_t,\theta$ that can generate any upper bounded horizon distribution, when $\theta$ is empirical gaussian and $A_t$ is a parametrization function for transformers. Lets suppose we wanted to use that knowledge to find an upper bound for the error of the optimal bayeisan optimizer e.g.  an upper bound of the mutual information H_t Theta. Can we do that by using the random random distribution $\Theta \sim \mathcal N(0,\sigma)$, as that way $\Theta$ is the probability distribution with maximal differential entropy? 
<!-- Not sure yet whether this is the actual problem -->
The problem is, that there exists $\theta \in \mathbb J^n$, s.t. $A(\theta) = A(\Theta)$.

The problem is, that this assumes, that $A$ is the only function (it is actually class of function) that can do that (explain what). Why
 suppose B (b entagles and makes dependencies) 



)
!!!!!!
But $A(\theta)$ only has horizon of $K<T$ and $A(\Theta)$ has infinite horizon (if A is paramatrization of transformers) and thus models and thus A can only exist for AMs with horizin at least T or with AMs that do not increase the horizon.  


This means only a bayesian estimator with horizon T can detect $\Theta$??


conclusion from A: for each prior distribution of Theta there is a constant theta that creates a prediction error of 0 but the same distribution of X_t



can A be inverted for arbitrarili long horizon case? by definition yes. so also for unbounded case we find good and shit model?
