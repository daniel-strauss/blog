---
title: Information Theory in LLM Theory
bibFile: data/bibliography.json # path relative to project root
---


## Introduction

## Background

 

### A Brief Introduction on Information Theory

In this section we want to provide a brief overview on Information Theory. 


Draft:
- start with everyday life example, that will make reader think of how one could formalize amount of information

- explain shannons information theory
    - name entropy definition
    - extract definition of information 
    - define mutual information, briefly mention why it is usefull


- briefly name an example applications from coding theory
- briefly name an example of neurology where neurons may maximize their entropy




### Philosofical Interlude: Does Information Exist?

Chapter Draft (everything in this draft is very vague, dont read)
- make reader know that you are critic of your own ideas
 - Previous Section information relied on an assumed probability space
 - what is probability? It is a way to formally express of what can be known and what can not be known? (<- express waeknasses of that thought and derive it and be more precise and present alternatives of expressong of what can be known and what not)
- Problems of probability:
    - How well can be known what can not be know? Well enough to express probabilitys?

    - maybe also everything can be known and everything happens with probability one (<- drop argument, why one can not be sure that universe is not deterministic universe, by using argument based on an explanation why even anything exists)
        - explain that under that circumstances only physically isolated can be known, using similar "proof" as holding theorems proof
        - include limitations of calculation power into probability?
- use previous arguments to express where probabilistic assumptions might  be off and name examples on how it could affect us practically
- conclude if we are carefull enough with probabilistic assumptions under scenario a everything has information 0, under scenario b everything has information $\infty$ and under scenario a using limitation of physical possible calculation power information content is impossibly hard to compute
- mention that information theory has been usefull anyway



## The Link of Information Theory and LLMs

## Analyzing Neural Network Architectures {#anal_nn}


![targets](/figures/assumption_for_ideal_bayesian_estimator.png "Assumption AM") 


In this chapter results from {{< cite "jeon2022information" >}} and {{< cite "jeon2024information" >}} will be discussed. For several generative models models upper bounds have been established of how much information they can generate. These results can be used make statements about the expressiveness of different neuronal architecture. (<- be more precise about expressivemness)



## An Information Theoretic Perspective Analysis on In-Context Learning

In {{< cite "jeon2024information" >}} assumptions about the origin of the trianing data of LLMs have been made from which an explanation of in context learning was stated.


### What is In-Context Learning


### Results and Methodology {{< cite "jeon2024information" >}}
sfg
#### Quick Overview


<!-- be consistent in the use of the word "bayesian prior" and "distribution of bayesian prior" -->

In [Analyzing Neural Network Architecture](#anal_nn) we discussed, how much information a given neural architecture can generate. The infered bounds can also be used to make statements about the training data probability space, if a certain assumption about that training data probability space was made. Which? The training data was generated by a random neural network of a given architecture. We will evaluate the pros and cons of such an assumption in [Discusion of their Assumptions](#discussion-assumptions). Once the distribution of the distribution (basically the bayesian prior) which generates the data  is formalized, we can make formal statements about the optimal (optimal with respect to a chosen loss function) estimator of the distribution of the training data. We call this estimator the optimal Bayesian estimator (OBE). If additionaly one assumes that a well trained transformer acts similarily as well as the oOBE, one can infer from the theoretic performance of the bayesian estimator on the performance on the transformer. When making these assumptions one has to be aware about certain dangers:
  - Is the bayesian prior distributio - the assumed distribution of distributions - plausible? Mor on that in [Discusion of their Assumptions](#discussion-assumptions)
  - The estiamtion of the OBE strongly depends on the bayesian prior distribution. If you change this distribution the OBE changes as well. If you assume transformers are really good, such that they are as good as an OBE, then the have to be as good as the OBE that makes correct assumptions on the bayesian prior

#### The Assumed Bayesian Prior



![targets](/figures/assumption_sparse_mixture.png "Assumptio Sparse Mixture") 


Figure 1: The model of {{< cite "jeon2024information" >}} of the training data for LLMs. Each square represents a training Document, which has been randomly generated by a sparse mixture of AMs like transformers. Each Pink circle represents a randomly generated AM and for each document a random AM is assigned based on a random random distribution.  



In this paragraph we will formally state the probabilistic model of {{< cite "jeon2024information" >}} for the generation of the training data and in-context window of LLMs.

We denote the $M$ as the number of training documents and the training documents with $\{D_1,...D_M\}$. $D_i$ is the sequence of tokens in the i'th document. $H_{m,t} := (D_1,...,D_{m-1}, X_1^{(m)},...,X_t^{(m-1)}) $ is an abreviation for the sequence of tokens created by the tokens in the first $m-1$ documents and the first $t$ tokens in the $m$'th document. $D_{M+1}$ denotes in-context document.

We say the distributions of $\{D_1,...D_M\}$ can described by autoregressive models (as are transformers) that are parametrized by the random vectors $\{\theta_1,...,\theta_m \}$.

The authors wanted to model $\theta_1,...,\theta_m $ such that they have some universal common information, which can be stored in a random variable $\psi$. This means that the sequence $\theta_1,...,\theta_m | \psi$ shall be iid. Additionally $\psi$ shall not contain information that could not be deducted from enough samples of $\theta_i$, e.g. $\lim_{n\to \infty} \mathbb H(\theta_{n+1} | \theta_1, ..., \theta_n) = \mathbb H(\theta_{n+1} | \psi)$(<- Check if this is correct) How can the random sequence $\theta_1,...,\theta_m$ be modeled to satisfy that constraint in such a way, that the distributions of $\theta_1,...,\theta_m$ and $\psi$ are well enough defined. In {{< cite "jeon2024information" >}} the authors came up with the clever solution for satisfying these constraints. They defined a random set of $N$ randomly initialized autoregressive models $T = \{\theta^{(1)},..., \theta^{(N)} \}$, where $N$ is an unknown number. Then they defined a random assignment of Documents and autoregresive models in T parametrized by random random distribution $\alpha \sim \text{Dirichlet}(N, (R/N, ..., R/N))$, with $R<<N$. $\alpha$ defines for a random autoregressive model $\theta^{(n)}$ its probability to be assigned for Documents. The smaller the values in the parameter-tuple $(R/N,...,R/N)$ of the Dirichlet distribution, the more sparse the distribution. Lets suppose for example if you have a factory that produces factories, which in return produce each $k$ dices and you want the dieces to be fair (each side appears with same probability). Then the dirichlet distribution, parametrized by $k, (a,...,a)$, which describes the distribution of the probability distribution dices created by a given dice factory should have a high value of $a$ in its paramatrization. <!--example is unintuitive + maybe be a bit more formal what alpha does--> 
So now we can finally define $\psi := (\alpha, \theta^{(1)}, ..., \theta^{(n)} )$. Note that $\theta_m | \psi$ is a discrete random variable with at most $N$ outcomes, therefore its entropy has an upper bound of $\log N$.

#### Results for in-context learning

In this paragraph we outline the results {{< cite "jeon2024information" >}} drew from this models of data generation, by analyzing the optimal bayesian estimator $\hat P$ for the probability distribution of $X^{(m)}_{t+1}$ given $H_t^{(m)}$.  

The optimal bayesian estimator is the estimator for the probability that minimizes this loss function:

$\mathbb L(P) = \frac{1}{TM} \sum_{m=1}^{M} \sum_{t=0}^{T-1} \mathbb E[ - \ln P(H_t^{(m)})(X_{t+1}^{(m)})]$

A little remark on the expression $P(H_t^{(m)})(X_{t+1}^{(m)})$: $P$ is a function that takes in an event like $H_t^{(m)}$, and returns a function, namely an estmated distribution for $X_{t+1}^{(m)}$. Therefore there are two braket pairs after $P$ in the above equation. 

In {{< cite "jeon2024information" >}} it was shown, that $\hat P(H_t^{(m)}) = x \to \mathbb P[x|H_t^{(m)}]$. Let's denote $\mathbb L := \mathbb L(\hat P)$.

We know


### Discusion of their Assumptions {#discussion-assumptions}


#### When can we make assumptions of the the familie of AMs

Lets suppose following scenario: we have a sequence of random tokens $X_1, ..., X_T$ and know that this sequence is autoregressive with finite horizon T-d, which means $\mathbb P(X_{t+1} | X_{t-T+d}, ..., X_{t-1}) = \mathbb P(X_{t+1}|H_t, X_{t+2}, ...X_{T})$.
Then, the distribution of $H_T$ can be charachterized by the function $f(H_t) = \mathbb P(X_{t+1} | H_t)$. Thus there is a trivial bijection from the set $\{ \Sigma^{T-d} \to \mathcal P_\Sigma \}$ to the Autoregressive distributions over the alphabet $\Sigma$ and length $T$ and horizon $T-d$. (<- warning you have to integrate the non character sign to sigma and that can only be place to the right position (change $\Sigma^T$)).

Lets suppose we are reely lucky and  find a parmatrization of all autoregressive distributions of horizon T-d by finding an injektive function $A :\mathbb J_\sigma^n \to \{\Sigma^{T-d} \to \mathcal P_\Sigma \}$, where $\mathbb J_\sigma ^n := \{||x||_2 = \sigma, \mathbb{1} ^T x=0 | x \in \mathbb R^n\}$.


(

Let now $\Theta \sim \mathcal N(0,I_n\sigma)$. Then $A(\Theta)$ is a random random distribution which itself is a random distribution. This means $A(\Theta) \in \{\Sigma^T \to \mathcal P_\Sigma \}$ (Note $X \sim \mathcal N(0,1) \to x \notin \mathbb R$) 



<!--This means there must exits $\theta \in \mathbb J^n$, such that  $A(\theta) = A^{-1}(A(\Theta))$-->

<!--Try to find argument that autogenerative model can not create itsel, can not be a finite horizon thing, but I think that is going nowhere-->


If A exists, and we  insert random vector into A, then that random variable is element of As image




<!--Say that iid if theta is not the worst case assumption for our model.-->

To recapulate, we assumed/showed evidence, that there can exist a pair $A_t,\theta$ that can generate any upper bounded horizon distribution, when $\theta$ is empirical gaussian and $A_t$ is a parametrization function for transformers. Lets suppose we wanted to use that knowledge to find an upper bound for the error of the optimal bayeisan optimizer e.g.  an upper bound of the mutual information H_t Theta. Can we do that by using the random random distribution $\Theta \sim \mathcal N(0,\sigma)$, as that way $\Theta$ is the probability distribution with maximal differential entropy? 
<!-- Not sure yet whether this is the actual problem -->
The problem is, that there exists $\theta \in \mathbb J^n$, s.t. $A(\theta) = A(\Theta)$.

The problem is, that this assumes, that $A$ is the only function (it is actually class of function) that can do that (explain what). Why
 suppose B (b entagles and makes dependencies) 



)
!!!!!!
But $A(\theta)$ only has horizon of $K<T$ and $A(\Theta)$ has infinite horizon (if A is paramatrization of transformers) and thus models and thus A can only exist for AMs with horizin at least T or with AMs that do not increase the horizon.  


This means only a bayesian estimator with horizon T can detect $\Theta$??


## Conclusion

In this blog post several applications of Information Theory in Deep Learning and LLMs where presented. 

## Take home messages


# References
{{< bibliography >}} 
