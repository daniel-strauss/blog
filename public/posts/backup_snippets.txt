d

## Analyzing Neural Network Architectures using Information Theory {#anal_nn}


![targets](/figures/assumption_for_ideal_bayesian_estimator.png "Assumption AM") 


Information theory can be found everywhere, where uncertainty is expressed probabilisticly. As language models typically learn a probability distribution for the next token given a previous sequence parts maybe some new insights on them may be gained by looking at them from the viewpoint of information theory. How much information can a generative model generate, e.g. how much information is stored in this model? Can they solve tasks related to information theory such as data compression? 

In this chapter results from {{< cite "jeon2022information" >}} and {{< cite "jeon2024information" >}} will be discussed. For several generative models models upper bounds have been established of how much information they can generate. These results can be used make statements about the expressiveness of different neuronal architecture. (<- be more precise about expressivemness)



#### Can a Transformer learn a random transforer as the OBE does

No, the AM represented by a random transformer has an event horizon of infinity.(<- TODO proof. youse math snippes from previous section>) Transformers only have a a


#### A sparce Mixture of Transformers has an infinite horizon

Suppose bernoulli distributed $\theta$ describing am with horizon of one. then $\theta$ together with am would create series with infinite horizon



Lemma A: Let $M_1 = (P_1,V)$ and $M_2 = (P_2,V)$ be markov chains, $P_1 \neq P_2$ and $\theta \sim Ber(a,b)$ and constants $a,b \in (0,1)$ and $a \neq b$. Let $x_0,X_1,X_2...$ be a random sequence in $V$, where $x_0$ is constant and $X_1, ...$ be described with probability $\theta$ by $M_1$ and with probability $1-\theta$ by $M_2$. Then the finite horizon $d$ optimal bayesian estimator $\mathbb P(X_{t+1}|X_{t-d+1}, X_{t-d+2} ..., X_t)$ does not equal the unbounded optimal bayesian estimator $\mathbb P(X_{t+1}|X_1, ..., X_t) $ for $t>d$.
(TODO you might be able to simplify that M_1 ~ Bin(a))
Proof: TODO

Below GRABAGE

Since $P_1 \neq P_2$ there must be $(x,y)\in V^2$, such that $P_1((x,y)) \neq P_2((x,y))$. Let $T_i$ denote the random variable that $M_\theta$ visits $x$ for the $i$'th time. As $M_1$ and $M_2$ are irreducible and finite for $i \in \mathbb N$, $T_i$ is finite with probability 1. (TODO proof, argument?) Note that $X_{T_i}(\omega) = x$.

Now:

EQ1: $\mathbb P(X_{T_i +1} = y| H_{T_i}) = P_1(x,y) \mathbb P(M_\theta = M_1|H_{T_i}) + P_2(x,y) \mathbb P(M_\theta = M_2|H_{T_i})$

  | Lets find a simpler expression for $P(M_\theta = M_1|H_{T_i})$:
  $P(M_\theta = M_1|H_{T_i}) = $

----


$= P_1(x,y) \mathbb P(M_\theta = M_1|H_{T_i}) + P_2(x,y) \mathbb P(M_\theta = M_2|H_{T_i})$


<!---
Since $P_1 \neq P_2$ and $M_1$ and $M_2$ are irreducible there is $t'$, such that $P_1((X_{t'},X_{t'+1})) \neq P_2((X_{t'},X_{t'+1}))$. For simplicity we define $\hat P_1 := P_1((X_{t'},X_{t'+1}))$ and $\hat P_2 := P_2((X_{t'},X_{t'+1}))$.
Lets analyze this probability:

$\mathbb P(X_{t'+1}| H_{t'}) = \mathbb P(X_{t'} | X_{t'+1}, M_\theta =  M_1) \cdot \mathbb P(M_\theta =  M_1 | H_{t'}) + \mathbb P(X_{t'} |X_{t'+1}, M_\theta =  M_2) \cdot \mathbb P(M_\theta =  M_2 | H_{t'})$

$= \hat P_1 \cdot \mathbb P(M_\theta =  M_1 | H_{t'}) + \hat P_2 \cdot \mathbb P(M_\theta =  M_2 | H_{t'})$ = .

thus hat M_1 and hat M_2 have to be known (actually only a part of them has to be known)

-->
ABOVE GARBAGE



- write how cool it would be if we where able to find for a given X distribution, the bayesian prior that maximises the estimation error
  - would the resulting model then be the perfect model for this distribution?
    - this is the model that would minizie H(X|Theta)   
    - this means that for many samples better estimation performance? (no bayesian prior always perfekt, but we are not, maximal possible estimation error tells us how much randomness will be removed for many samples) 
  - for example uniform distribution forces estimation error to be zero, e.g. every modelling of uniform distribution is equal as shit
