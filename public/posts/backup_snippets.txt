d

## Analyzing Neural Network Architectures using Information Theory {#anal_nn}


![targets](/figures/assumption_for_ideal_bayesian_estimator.png "Assumption AM") 


Information theory can be found everywhere, where uncertainty is expressed probabilisticly. As language models typically learn a probability distribution for the next token given a previous sequence parts maybe some new insights on them may be gained by looking at them from the viewpoint of information theory. How much information can a generative model generate, e.g. how much information is stored in this model? Can they solve tasks related to information theory such as data compression? 

In this chapter results from {{< cite "jeon2022information" >}} and {{< cite "jeon2024information" >}} will be discussed. For several generative models models upper bounds have been established of how much information they can generate. These results can be used make statements about the expressiveness of different neuronal architecture. (<- be more precise about expressivemness)



#### Can a Transformer learn a random transforer as the OBE does

No, the AM represented by a random transformer has an event horizon of infinity.(<- TODO proof. youse math snippes from previous section>) Transformers only have a a


#### A sparce Mixture of Transformers has an infinite horizon

Suppose bernoulli distributed $\theta$ describing am with horizon of one. then $\theta$ together with am would create series with infinite horizon



Lemma A: Let $M_1 = (P_1,V)$ and $M_2 = (P_2,V)$ be markov chains, $P_1 \neq P_2$ and $\theta \sim Ber(a,b)$ and constants $a,b \in (0,1)$ and $a \neq b$. Let $x_0,X_1,X_2...$ be a random sequence in $V$, where $x_0$ is constant and $X_1, ...$ be described with probability $\theta$ by $M_1$ and with probability $1-\theta$ by $M_2$. Then the finite horizon $d$ optimal bayesian estimator $\mathbb P(X_{t+1}|X_{t-d+1}, X_{t-d+2} ..., X_t)$ does not equal the unbounded optimal bayesian estimator $\mathbb P(X_{t+1}|X_1, ..., X_t) $ for $t>d$.
(TODO you might be able to simplify that M_1 ~ Bin(a))
Proof: TODO

Below GRABAGE

Since $P_1 \neq P_2$ there must be $(x,y)\in V^2$, such that $P_1((x,y)) \neq P_2((x,y))$. Let $T_i$ denote the random variable that $M_\theta$ visits $x$ for the $i$'th time. As $M_1$ and $M_2$ are irreducible and finite for $i \in \mathbb N$, $T_i$ is finite with probability 1. (TODO proof, argument?) Note that $X_{T_i}(\omega) = x$.

Now:

EQ1: $\mathbb P(X_{T_i +1} = y| H_{T_i}) = P_1(x,y) \mathbb P(M_\theta = M_1|H_{T_i}) + P_2(x,y) \mathbb P(M_\theta = M_2|H_{T_i})$

  | Lets find a simpler expression for $P(M_\theta = M_1|H_{T_i})$:
  $P(M_\theta = M_1|H_{T_i}) = $

----


$= P_1(x,y) \mathbb P(M_\theta = M_1|H_{T_i}) + P_2(x,y) \mathbb P(M_\theta = M_2|H_{T_i})$


<!---
Since $P_1 \neq P_2$ and $M_1$ and $M_2$ are irreducible there is $t'$, such that $P_1((X_{t'},X_{t'+1})) \neq P_2((X_{t'},X_{t'+1}))$. For simplicity we define $\hat P_1 := P_1((X_{t'},X_{t'+1}))$ and $\hat P_2 := P_2((X_{t'},X_{t'+1}))$.
Lets analyze this probability:

$\mathbb P(X_{t'+1}| H_{t'}) = \mathbb P(X_{t'} | X_{t'+1}, M_\theta =  M_1) \cdot \mathbb P(M_\theta =  M_1 | H_{t'}) + \mathbb P(X_{t'} |X_{t'+1}, M_\theta =  M_2) \cdot \mathbb P(M_\theta =  M_2 | H_{t'})$

$= \hat P_1 \cdot \mathbb P(M_\theta =  M_1 | H_{t'}) + \hat P_2 \cdot \mathbb P(M_\theta =  M_2 | H_{t'})$ = .

thus hat M_1 and hat M_2 have to be known (actually only a part of them has to be known)

-->
ABOVE GARBAGE



- write how cool it would be if we where able to find for a given X distribution, the bayesian prior that maximises the estimation error
  - would the resulting model then be the perfect model for this distribution?
    - this is the model that would minizie H(X|Theta)   
    - this means that for many samples better estimation performance? (no bayesian prior always perfekt, but we are not, maximal possible estimation error tells us how much randomness will be removed for many samples) 
  - for example uniform distribution forces estimation error to be zero, e.g. every modelling of uniform distribution is equal as shit


### Can a transformer implement a sparse mixture of transformers?

!!IMPORTANTE IMPORTANTE!!: adapt notation to Jeon et al 2024!!!!!

(No a sparce mixture of transformers has an infinite horizon.)

In this section, we show that a sparce mixtrue of AMs can not be expressed by a set any finite horizon AM model. Furthermore we hypothesize which 

----
#### Definition: 
Let $A$ be a set and $n \in \mathbb N$, then **$A^{[n]} := \bigcup_{i\in \{0,1,...,n\}} A^i$**. 

----


$A^i$ can be seen as the set of all words of length $i$ over alphabet/tokenset $A$ and consequently $A^0$ as the set, that contains the empty word, given $A$ is not the empty set. 

----

#### TODO Define s_1, d_1

---

#### Definition: 
Two makrov-chains $M_1 = (P_1,V)$ and $M_2 = (P_2,V)$ are called **deterministically distinguishable (dd)** if there is transition $e \in V^2$, such that $P_1(e) + P_2(e)>0$ and $P_1(e) \cdot P_2(e) = 0$. (E.g. $M_1$ and $M_2$ are dd if one has a transition with probability zero, where the other has positive transition probability)

----

#### Lemma A: 
Let $M_1 = (P_1,V)$ and $M_2 = (P_2,V)$ be not dd but irreducible and finite makrov chains, $P_1 \neq P_2$ and $\theta \in (0,1)$. Let $x_0,X_1,X_2...$ be a random sequence in $V$, where $x_0$ is constant and the distribution of $x_0, X_1, ... $ follows with probability $\theta$, $M_1$ and with probability $1-\theta$, $M_2$. Then there is $t*$ such that for infinite $t>t*$ the finite horizon $d$ optimal bayesian estimator $\mathbb P(X_{t+1}|X_{t-d+1}, X_{t-d+2} ..., X_t)$ does not equal the unbounded optimal bayesian estimator $ \mathbb P(X_{t+1}|X_1, ..., X_t) $.

(TODO maybe you can replace irreduciblt with something weaker)
TODO make this : we assume M_theta = M_1 more formal somehow, TODO define more clearley what doe not equal 
#### Proof: 
We cdenote the random markov cahin, that is $M_1$ with probability $\theta$ and otherwise $M_2$, with $M_\theta$. We first show two statemetns:
- a) Probability of $M_\theta = M_1$ grows with t to one given sequenze H_t and if $M_1 = M_\theta$
- b) Probability of $M_\theta = M_1$ less than one, given t is less than one.

- a) $\lim_{t \to \infty} \mathbb P[M_\theta = M_1 | H_t] = 1$, if we assume $M_\theta = M_1$
- b)  For all $t \in \mathbb N$ it holds: $\mathbb P[M_\theta = M_1 | H_t] < 1$, if we assume $M_\theta = M_1$



##### Proof of a)  


Since $P_1 \neq P_2$ there must be $(x,y)\in V^2$, such that $P_1((x,y)) \neq P_2((x,y))$. Let $T_i$ denote the random variable that $M_\theta$ visits $x$ for the $i$'th time. As $M_1$ and $M_2$ are irreducible and finite for $i \in \mathbb N$, $T_i$ is finite with probability 1. (TODO proof, argument?) Note that $X_{T_i}(\omega) = x$. For convenience we denote $p_1 := P_1(x,y)$ and $p_2 := P_2(x,y)$.

Notice that the sequence $X_{T_1 +1}, X_{T_2 +1}, ... | M_\theta$ is iid. To make our life even more easier we define the binomial variable $Y_i = \bold 1[X_{T_i + 1} = y]$. Now $Y_1, Y_2... $ is an infinite sequence of coinflips, that land with probability $\theta$ with probability $p_1$ on heads and with probability $1-\theta$ with probability $p_2$ on heads.

Let $A_t = \bold 1 [|\dfrac{S_y}{t} - p_1| < |\dfrac{S_y}{t} - p_2|] $


$A_t \to_{a.s.} 1$ iif $M_1 = M_\theta$ (todo show)

proof of a) finished

##### Proof of b)

Broof by induction:
Assume $M_1 = M_\theta$, assume for $t \in \mathbb N$ it holds: $\mathbb P[M_\theta = M_1 | H_t] = p_t < 1$.

Then $\mathbb P[M_\theta = M_1 | H_t, X_{t+1}] = \dfrac{\mathbb P[M_\theta = M_1|H_t]}{\mathbb P[ X_{t+1}| H_t]}\mathbb P[X_{t+1} | M_\theta = M_1,H_t] $

$= \dfrac{p_t P_1(X_t, X_{t+1})}{p_tP_1(X_t, X_{t+1}) + (1-p_t)P_2(X_t, X_{t+1})} <1$


Proof of b) finished.

(todo make limit more formal, make this whole block below more formal!!)
As we know from a) $\lim_{t\to \infty} \mathbb P(X_{t+1}|H_t) = \lim_{t\to \infty} \mathbb P(X_{t+1}|H_t, M_\theta = M_1) = \lim_{t \to \infty} P(X_{t+1}|X_t, M_\theta = M_1) = P_1((X_{t+1},X_t))$. (<- be more formal)
But we also know from b) (TODO deal with non existance of d in b))that there is $0<p_t < 1$, s.t. $\lim_{t \to \infty} \mathbb P(X_{t+1}|X_{t-d+1},...,X_t ) = \lim_{t \to \infty} p_t \cdot P_1(X_t, X_{t+1}) + (1-p_t) \cdot P_2(X_t, X_{t+1})$. And as defined in the lemma there is x,y such that $P_1(x,y) \neq P_2(x,y)$ 

QED

---

Lemma A means the resulting probability distribution of $x_0, X_1, ... $ made in this simple construction can not be expressed by any finite horizon AM model, such as a transformer

----

#### Hypothesis B: 
when d grows to infinity posterior distribution can be epsilon approximated

Proof: Left out

----

#### Corolary of lemma A: 
An upper bounded context $d_1$ length Transformer $T$ can not express the probability distribution a non-deterministic mixture of two transformers $(R,J)$ (Romeo and Juliett).

#### Proof: 
We start by showing the possibility of a reduction: Any discrete autoregressive distribution over alphabet $\Sigma$ with horizon $d$, $f:\Sigma^{[d]} \to \mathcal P_{|\Sigma|}$ can be express by an equivalent Markov chain $M=(V_m,P_m)$, where $V_m = \Sigma^{[d]}$. With equivalent it is meant that the new random model, has the same irreducible error, and the same bayesian estimation error (todo be formal about what you mean with equivalent (equivalen with respekt to obe), explain meaning of f!)
To achieve the epuivalent markov chain, we construct $P_m$ like this: given $w \in \Sigma^{[d]}$, $x \in \Sigma$ and $f(w)(x) = p$, $P_m((w, s_1(w) \circ x)) = p$. If a transition $(a,b) \in V_m$ has not defined in the previous sentence (e.g. $s_1(a) \neq p_1(b)$), then its porbability is zero $P_m((a,b)) = 0$.

(TODO: adapt to new version of lemma A, e.g. chains are not dd and irreducible)
Therefore there exists an equivalent markov chain $M_R=(V,P_R)$  of $R$ and $M_J$ analogously of $J$. By lemma A the distribution of a non-deterministic mixture of $M_R$ and $M_J$ can not be approximated by a bounded optimal bayesian optimizer. But as the parameters of $T$ has context length $d$ it can only express distributions of auto regressive models with horizons of at most $d$. Thus $T$ can not implement the non-deterministic misxture of R and J.


QED 

----

#### Corolary of corolary of lemma A: 
An upper bounded context $d_1$ length Transformer $T$ can not express the probability distribution a non-deterministic mixture of n transformers $\{J_1,...,J_n\}$ for any n>2.

#### Proof (TODO)

Assume ad absurdum: asumme n>0 exists, st a mixture of n fhAms can be implemented by an fhAM.
TODO finish proof
  - reduction of determening of fhAM_a, fhAM_b to determening n fhAMs.
  - generate n-2 random fhAMs (fhAM_2, ..., fhAM_n), and generate sequence of tokens with p=0.5 by (fhAM_a, fhAM_b) and with 0.5 percent by (fhAM_2, ..., fhAM_n) 
  - this would mean after d tokens obe must know whether it is fhAM_a and fhAM_b or not
  - given that obe knows 

 (TODO in definition of non deterministic mixture) (Romeo and Juliett).

QED

----

#### Comments: 
We assumed in corolarry of Lemma A that R,J take every next token with positive probability. This assumption was necessary for the corrolary to be true in general. Imagine R would give for a word $w$ to be followed by a letter $x$ the probability zero and T not. Then the optimal estimator would disguise T as soon as x would follow after w. Does that mean it stays plausible that for a non deterministic mixture of tansfoermes, that are limited to only taking one of the k most likely letters as the next one,  to be implemented as one transformer. I believe only yes, if all parameters of R and J are perfektly well known. As soon there is some ambiguity in there parameters, suddenlty we can not be exactly sure wich one is among the top k next tokens and suddently again all tokens could appear with positve probability. But thats only a believe and I do not provide any qualitative argument on that.

Also if we assume that 


#### Corolary of corolary of lemma A: 
An upper bounded context $d_1$ length Transformer $T$ can not express the probability distribution a non-deterministic mixture of n transformers $\{J_1,...,J_n\}$ for any n>2.

#### Proof (TODO)

Assume ad absurdum: asumme n>0 exists, st a mixture of n fhAms can be implemented by an fhAM.
TODO finish proof
  - reduction of determening of fhAM_a, fhAM_b to determening n fhAMs.
  - generate n-2 random fhAMs (fhAM_2, ..., fhAM_n), and generate sequence of tokens with p=0.5 by (fhAM_a, fhAM_b) and with 0.5 percent by (fhAM_2, ..., fhAM_n) 
  - this would mean after d tokens obe must know whether it is fhAM_a and fhAM_b or not
  - given that obe knows 

 (TODO in definition of non deterministic mixture) (Romeo and Juliett).

QED

----

#### Corolary Theorem A.2: 
An upper bounded context $K_t$ length Transformer $T$ can not express the probability distribution a non-deterministic mixture of n transformers $\{J_1,...,J_n\}$ for any n>2.

#### Proof (TODO)

Assume ad absurdum: asumme n>0 exists, st a mixture of n fhAms can be implemented by an fhAM.
TODO finish proof
  - reduction of determening of fhAM_a, fhAM_b to determening n fhAMs.
  - generate n-2 random fhAMs (fhAM_2, ..., fhAM_n), and generate sequence of tokens with p=0.5 by (fhAM_a, fhAM_b) and with 0.5 percent by (fhAM_2, ..., fhAM_n) 
  - this would mean after d tokens obe must know whether it is fhAM_a and fhAM_b or not
  - given that obe knows 

 (TODO in definition of non deterministic mixture) (Romeo and Juliett).

QED