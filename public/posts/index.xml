<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Daniels Blog</title>
    <link>https://strammermax27.github.io/posts/</link>
    <description>Recent content in Posts on Daniels Blog</description>
    <generator>Hugo -- 0.128.2</generator>
    <language>en-us</language>
    <atom:link href="https://strammermax27.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Information Theory in LLM Theory</title>
      <link>https://strammermax27.github.io/posts/information_theory_in_llm_theory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://strammermax27.github.io/posts/information_theory_in_llm_theory/</guid>
      <description>⚠️ 📥 😚 🛡 🚦 👹 🌿 By reading this blogpost, you will find out, why this emoji sequence is here.
Introduction Background A Brief Introduction on Information Theory In this section we want to provide a brief overview on Information Theory.
Information theory is the study on expressing the quantity of information called entropy. Entropy of an information is the minimal espected amount of bits (todo explain that it doesnt have to be bits) required to encode this information.</description>
    </item>
  </channel>
</rss>
