<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Daniels Blog</title>
    <link>https://strammermax27.github.io/</link>
    <description>Recent content on Daniels Blog</description>
    <generator>Hugo -- 0.129.0</generator>
    <language>en-us</language>
    <atom:link href="https://strammermax27.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Some Information Theoretic Perspectives on LLM Theory</title>
      <link>https://strammermax27.github.io/posts/information_theory_in_llm_theory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://strammermax27.github.io/posts/information_theory_in_llm_theory/</guid>
      <description>⚠️ 📥 😚 🛡 🚦 👹 🌿 By reading this blogpost, you will find out, why this emoji sequence is here.
TODO rewrite thetas, random theta: big, constant theta small
Introduction In this Blogpost you will first learn about information theory and we will look at one example application of information theory in llm theory. Then we will look at one paper in detail, that looked specifically at the phenomenon of in-context learning by making assumptions about the probability distribution of the training data.</description>
    </item>
  </channel>
</rss>
