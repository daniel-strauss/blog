<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Daniels Blog</title>
    <link>https://strammermax27.github.io/</link>
    <description>Recent content on Daniels Blog</description>
    <generator>Hugo -- 0.128.2</generator>
    <language>en-us</language>
    <atom:link href="https://strammermax27.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Some Information Theoretic Perspectives on LLM Theory</title>
      <link>https://strammermax27.github.io/posts/information_theory_in_llm_theory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://strammermax27.github.io/posts/information_theory_in_llm_theory/</guid>
      <description>⚠️ 📥 😚 🛡 🚦 👹 🌿 By reading this blogpost, you will find out, why this emoji sequence is here.
TODO rewrite thetas, random theta: big, constant theta small
Introduction In this Blogpost you will learn about some papers, that explored LLMs through the lens of Information Theory. We will look at one paper in detail, that looked specifically at the phenomenon of in-context learning by making assumptions about the probability distribution of the training data.</description>
    </item>
  </channel>
</rss>
