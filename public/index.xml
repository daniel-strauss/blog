<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blog of Daniel Strauss</title>
    <link>https://daniel-strauss.github.io/blog/</link>
    <description>Recent content on Blog of Daniel Strauss</description>
    <generator>Hugo -- 0.133.0</generator>
    <language>en-us</language>
    <atom:link href="https://daniel-strauss.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Some Information Theoretic Perspectives on LLM Theory</title>
      <link>https://daniel-strauss.github.io/blog/posts/information_theory_in_llm_theory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://daniel-strauss.github.io/blog/posts/information_theory_in_llm_theory/</guid>
      <description>Author: Daniel StrauÃŸ, Supervisor: Suvrit Sra
ğŸ³ ğŸ˜¯ ğŸ”² ğŸ“¥ ğŸš¤ ğŸŒ§ By reading this blogpost, you will find out why this emoji sequence is here.
Introduction In this blog post, you will first learn about information theory, and we will look at one example of the application of information theory in LLM theory. Then, we will look at one paper in detail that specifically examines the phenomenon of in-context learning by making assumptions about the probability distribution of the training data.</description>
    </item>
  </channel>
</rss>
